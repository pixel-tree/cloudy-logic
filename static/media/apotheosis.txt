Men at some time are masters of their fates;
The fault, dear Brutus, is not in our stars,
But in ourselves, that we are underlings.
Cassius in Julius Caesar (Shakespeare)
“Apotheosis” refers to the act of playing God or to the making of something into one. This is a fitting analogy. Let us imagine a system, represented here as a three-dimensional object, a cube, the particles inside of which appear exactly as determined in the code; and their behaviour directed by a written sequence of instructions—an algorithm. The emergence of patterns (however complex) is, thus, simply the result of an explicitly defined set of rules. This is a trivial example of a deterministic system: change may be produced by manipulating the initial parameters (try the WASD keys).
A major challenge in predicting the future of real-world systems is that they are incredibly dynamic, complex and full of noise; also, rarely isolated and therefore susceptible to unforeseeable behaviours emerging. A system is hard to reduce to a function. For example, think of artificial neural networks with billions of parameters, which are able to learn highly accurate representations at various levels of abstraction but might still perform poorly when confronted with out-of-sample data. We also have limited access to the internal workings of machines learning, as the mathematical operations generally take place in high-dimensional vector spaces; although, we may visualise these to an extent and produce representations interpretable to humans (using PCA, t-SNE, matrix factorisation, etc.). Regardless, optimising for a model to perform well is mainly based on arbitrary metrics in highly specific contexts and on an intuitive understanding of the technology; the development often a heuristic process rather than something immediately obvious out-of-the-box.
Prediction technologies have advised human communities for thousands of years: Neolithic wayfarers sought informative signals and patterns in their surroundings; Babylonians produced weather forecasts and recorded them on stone; Greeks prophesied the will of the Gods through animal trails and frenzied rituals; and astronomy has been practiced by indigenous peoples around the planet. However, it was the Enlightenment paving way for causal determinism, and perhaps one of the most recognisable variations of this idea came from mathematician Pierre-Simon de Laplace in 1814:
“We may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at a certain moment would know all forces that set nature in motion, and all positions of all items of which nature is composed, if this intellect were also vast enough to submit these data to analysis, it would embrace in a single formula the movements of the greatest bodies of the universe and those of the tiniest atom; for such an intellect nothing would be uncertain and the future just like the past would be present before its eyes.”
You might recognise this as Laplace’s demon. More recently, we have discovered strange phenomena that challenge this postulate: namely, the quantum world. If particles also behave like waves, how can anything be predicted (when their position is unknown in the first place)? Werner Heisenberg therefore posited, in 1927, that the universe is knowable only with some degree of uncertainty. This becomes less of an issue when dealing with things at human-scale, as macroscopic objects seem to abide Newtonian laws. The connection to AI here is somewhat loose; but it highlights the historical tension between logic and uncertainty. Powerful priors (assumptions) help machine learning algorithms to generalise by guiding them in their learning. These may be expressed implicitly or explicitly: as the choice of one algorithm over another or a probability distribution that influences the model in some way. Rather paradoxically, explicit representations of knowledge are a way of encoding uncertainty into a model—a seemingly rational approach in light of an inherently probabilistic world in nature.
“The actual science of logic is conversant at present only with things either certain, impossible, or entirely doubtful, none of which (fortunately) we have to reason on. Therefore the true logic for this world is the calculus of Probabilities, which takes account of the magnitude of the probability which is, or ought to be, in a reasonable man’s mind.”
(James Clerk Maxwell)
Suppose that a vast majority of existing machine learning models are highly specialised: each designed to address a specific problem in rather narrow domains (image classification, speech recognition, etc.; for medical diagnostics, autonomous vehicles, etc.). We may zoom in further and categorise these models in various ways, into paradigms and architectures (unsupervised, supervised, etc.; CNNs, RNNs, vanilla DNNs, etc.); the semantics of which can be contested, but thinking about machine learning systems in more abstract terms we may divide a great proportion of these into three main components: 1) data representation, 2) objective function and 3) optimisation method. Regardless of the level of abstraction, we have chosen a way to represent data (i.e. vectors), something meaningful that we want to optimise for (cross entropy loss, reward, etc.) and a way in which to find our parameters (gradient descent, policy, etc.). In many cases, unless exclusively trying to better understand patterns within a specific set or problem (i.e. interpolate), our goal is to find a function with which to extrapolate beyond our original dataset (ability to generalise; ability deal with uncertainty; ability to extract meaningful information from new data). And this is where things become murky. Why are test results hard to reproduce? And why might a reasonably accurate model suddenly start malfunctioning when deployed into a comparable real-world context to that in which it was trained?