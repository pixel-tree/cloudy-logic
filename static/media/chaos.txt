“In every moment of her duration Nature is one connected whole; in every moment each individual part must be what it is, because all the others are what they are; and you could not remove a single grain of sand from its place, without thereby, although perhaps imperceptibly to you, altering something throughout all parts of the immeasurable whole.”
Johann Gottlieb Fichte
You might have heard of the expression: the flap of a butterfly’s wings in Brazil can set off a tornado in Texas. Whilst this may be impossible to prove, chaos theory generally applies to systems which are 1) dynamical, meaning that their behaviour at one point in time influences their future state and 2) nonlinear, meaning that they abide by exponential relationships. Therefore, chaotic systems are characterised by a sensitive dependence on initial conditions.
A neural network, for example, certainly ticks both boxes. In the forward propagation the outputs from one layer become inputs for the next; and the backward propagation consists of computing partial derivatives of a loss function with respect to the parameters of the network. Secondly, without nonlinear operations (activation functions) there would be little point in training deep networks: we try to model complex representations, and linear functions fail to provide any flexibility (pun intended). This is not to say that machine learning systems behave randomly but, rather, in an incredibly precise and intricate manner.
The upside of exponential operations is that they allow us to disentangle complex features; but they are punishing for any inaccuracies. Alas, within large networks what might seem like insignificant artefacts in data may be amplified to have profound impacts on the outcome. Imagine multiplying 10 by 10, and accidentally replacing one of the values with 9 or 11. The margin of error is +-10%. Not ideal by any means, but pales into insignificance when compared to making the same mistake raising 10 to the power of 10. In mis-keying our exponent, the result gives 10% or 1000% of the correct value. Add more depth to the network, and the mistake grows exponentially.
Data is often pre-processed in various ways to minimise the potential for these errors to emerge, and regularisation is used to prevent overfitting (the model being unable to generalise beyond the training data); but this does little when confronted with messy and dynamic real-world systems. With a range of measurement technologies, manipulation techniques, algorithms and apparatuses used for capturing, homogenising, cleaning and maintaining data, among other things, the possibilities for errors to enter the system are multiform. Moreover, as an iterative process any unwanted feedback will continue to increase in perpetuity. Seemingly trivial assumptions may distort the model and lead to significant deviation from the expected behaviour.
To make matters more complex: aside from conceptual innovation, deep learning research has advanced significantly by throwing more compute at problems. The reason being, presumably, that it works; but the trade-off is an increase in operational complexity. Intuitively, larger networks make sense. Think back to our RGB example. Instead of being restricted to categorising colour into black and white, or some shades of grey in between, there now exists a vocabulary of over sixteen million possible information states. For the sake of an argument, suppose that a neuron outputs a binary value (0, 1). A network with a higher number of neurons has a larger capacity to capture detail and to express the features of an object. Additionally, as more depth equals an increase in nonlinearity, we may find representations at higher levels of abstraction. In reality, neural networks are not as black and white as depicted here. Nevertheless, the point should be clear: the information (internal state of the network) becomes richer and more useful but, subsequently, more challenging to make sense of.
Cue: curveball. To an extent, bias is good (however counterintuitive this may seem considering the tone of public discourse). Inductive bias is an intrinsic property of effective deep learning: assumptions, as prior knowledge, are useful for distinguishing signal from noise. However, this comes paired with a design responsibility: although information might be (almost) uniformly phenomenal for humans, it is encountered in a state of semantic superposition. So how to design AI responsibly? The world is currently trying to figure this out. For now, there seems to be a significant gap between the technical logic and the business logic of AI: a dissonance between the work emerging from research labs and what the technology is being applied to out in the world. We face grand challenges to ensure inclusivity and fair worldbuilding; and in discussing bias, as implied in these writings, without specificity it is merely reduced to rhetoric—the scapegoat in a more expansive problem.